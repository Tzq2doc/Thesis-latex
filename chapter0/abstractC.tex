%# -*- coding:utf-8 -*-\begin{abstract}随着近年来众多大规模图像和视频数据集的出现，基于深度学习的计算机视觉技术取得了长足的进步。然而，对于复杂视觉场景的感知和理解，目前的计算机模型的表现与人类的表现还相差甚远，远远没有达到大规模普及和落地应用的水平。另一方面，日常生活中的视觉媒体数据通常都是包含复杂的视觉场景内容。为了充分地利用这些海量的视觉媒体数据，便利人们的日常生活，复杂视觉场景的感知和理解已经逐渐成为近年来计算机视觉领域的一个研究热点。本文将针对四个不同层次的视觉场景理解（包括物体识别、场景识别、场景理解和场景推理），逐步地对复杂视觉场景中视觉内容的识别、检测和推理进行研究。本文的关键技术线路主要聚焦于零样本物体分类、图像场景图生成、图像描述生成、视频片段检索和视觉问答等具体视觉场景理解任务。在此研究技术路线下，本文主要的研究内容和贡献如下：1）针对目前零样本物体分类模型中普遍存在的语义丢失的问题，本文提出一种全新的零样本学习网络。该网络首次引入两个相互独立的映射网络分支，将图像分类和图像重建两个原本相互冲突的任务分离出来。同时借助对抗学习，实现重建网络分支和分类网络分支之间的属性迁移，减缓模型的语义丢失的问题。2）针对目前图像场景图生成模型中优化目标通常忽略不同物体的重要性差异的问题，本文提出一种全新的训练框架，首次将场景图生成任务转化成多智能体协同决策问题，从而可以直接将整个场景图质量作为模型的优化目标，极大地提升场景图生成质量。同时，本文还提出了一种反事实基准模型，可以有效地计算每个物体类别预测对整体场景图的局部贡献，进而为模型训练提供更加高效的优化梯度。3）参考现有的空间注意力机制，本文首次提出通道注意力机制。同时，通过充分挖掘卷积神经网络的特征图的三个不同维度（空间维度、通道维度、层级维度）之间的联系，提出一种全新的空间和通道注意力网络。在图像描述生成任务中，该网络可以在生成文本的过程中动态地关注不同的空间区域和通道，不仅极大地提升了描述语句的生成质量，同时帮助人们理解在语句生成过程中特征图的变化过程。4）针对目前视频片段检索任务中两种主流框架（自顶向下和稀疏型自底向上）的设计缺陷，本文提出了一种全新的密集型自底向上的框架。通过将动作边界定位问题分解成相关性预测和边界回归两个子问题，显著地降低了动作边界定位的难度。同时，本文提出一个基于图卷积的特征金字塔层，来进一步增强骨干网络编码能力。该框架可以明显地提升多种视频片段检索任务的准确率。5）针对目前视觉问答模型忽略的两个重要特性（视觉可解释性和问题敏感性），本文提出了一种通用的反事实样本生成机制。通过遮盖图像中的重要区域或问题中的重要单词，同时更改标准答案，来生成大量的反事实样本。通过使用原始训练样本和新生成的反事实样本一起对视觉问答模型进行训练，迫使模型关注被遮盖的重要内容，提升模型的视觉可解释性和问题敏感性。\keywords{复杂视觉场景理解、零样本物体分类、图像场景图生成、图像描述生成、视频片段检索、视觉问答}\end{abstract}