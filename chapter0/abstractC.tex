%# -*- coding:utf-8 -*-\begin{abstract}随着近年来众多大规模人工标注的图像和视频数据集的出现，基于深度学习的计算机视觉技术取得了长足的进步。然而，对于复杂视觉场景的识别和理解，目前的计算机视觉模型的表现与人类的表现还相差甚远，远远没有达到落地应用和大规模普及的水平。但是，日常生活中的媒体数据通常都是复杂视觉场景。为了充分利用日常生活中的海量媒体数据，复杂视觉场景的感知和理解已经逐渐成为近年来计算机视觉领域的一个研究热点。本文将针对这四个不同层次的场景识别和理解（物体识别、场景识别、场景理解和场景推理），逐步地对复杂视觉场景的识别、检测和推理进行研究。本文的关键技术线路主要研究零样本物体分类、图像场景图生成、图像描述生成、视频片段检索和视觉问答等具体研究任务。在此研究路线下，本文主要的研究内容和贡献如下：\begin{asparaenum}\item 针对目前零样本物体分类模型中普遍存在的属性丢失的问题，本文提出一种全新的零样本学习网络：基于属性保持的对抗网络。该网络通过引入两个独立的映射网络分支，将图像分类和图像重建两个原本相互冲突的任务分离出来。然后利用对抗网络学习让重建网络的特征向量的部分属性能够迁移到分类网络的特征向量中，从而使得分类网络的特征向量保持尽可能多的属性，减缓语义丢失的问题。\item 针对流行的图像场景图生成的优化目标（物体和视觉关系分类的交叉熵之和）忽略不同物体重要性的问题，本文提出一种全新的训练框架，将图像场景图生成问题看成一个多智能体协同决策问题。基于新的框架，我们可以直接使用最终的场景图生成质量作为优化目标。同时，本文提出一种全新的反事实基准模型，来近似目标智能体预测类别的局部贡献。不仅可以显著地提升物体的类别准确率，同时提升整个场景图的生成质量。\item 对于图像描述生成，基于现有的空间注意力机制，本文提出一种全新的多层空间和通道注意力网络。通过充分挖掘卷积神经网络的特征图的三个维度之间的联系，使得模型在生成文本的过程中不断的关注不同的空间区域和通道。该网络不仅是一种编码能力更强的注意力网络，同时帮助人们理解在图像描述生成过程中卷积神经网络的特征图的变化过程。\item 对于视频片段检索，针对稀疏型自底向上模型的设计缺陷，本文提出一种全新的密集型自底向上的框架。通过将边界预测问题分解成相关性预测和边界回归两个子问题，大大降低了模型对视频动作边界定位的难度。同时，本文提出一个基于图卷积的特征金字塔层，来增强骨干网络编码能力。该框架大大提升了视频片段检索的准确率。\item 针对目前视觉问答模型忽略的两个重要特性（视觉可解释性和问题敏感性），本文提出了一种通用的反事实样本生成机制。通过人为地遮盖图像中的重要区域或问题中的重要单词，同时更改标准答案，生成大量的反事实样本。然后合并原始的训练样本和新生成的反事实样本可以得到全新的训练样本。通过使用全新的训练样本对视觉问答模型进行训练，迫使模型关注反事实样本中被遮盖的重要内容，提升视觉问答模型的准确率和鲁棒性。\end{asparaenum}\keywords{复杂场景理解，零样本物体分类，图像场景图生成，视觉描述生成，视频片段检索，视觉问答}\end{abstract}