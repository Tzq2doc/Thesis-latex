%# -*- coding:utf-8 -*- 
\begin{englishabstract}

Although computer vision techniques have achieved impressive progress over the last few years, today's computer vision models still perform far behind humans, and cannot be applied in our daily life on a large scale, especially for complex visual scene understanding. To fully take advantage of the massive amounts of data in our daily life, complex visual scene understanding is becoming one of the hottest research topics in computer vision.

In this thesis, I focus on visual recognition, detection, and reasoning for complex visual scene understanding. Specifically, I try to realize visual scene understanding from four different levels, including instance-level recognition, scene-level recognition, scene-level understanding, and scene-level reasoning. The related tasks in this thesis consist of zero-shot visual recognition, scene graph generation, image captioning, query-based video localization, and visual question answering. In summary, I make five main contributions in this thesis:

1) For the ubiquitous semantic loss problem in current zero-shot visual recognition models, I propose a novel zero-shot learning network: SP-AEN. The SP-AEN introduces two independent embedding networks for image classification and reconstruction respectively. Thanks to this design, the SP-AEN can disentangle these two conflict tasks. Meanwhile, the SP-AEN resorts to adversarial learning to transfer attributes from reconstruction embeddings to classification embeddings, which helps to preserve semantics in the classification embeddings and mitigate the semantic loss problem.

2) To identify the local contributions of different objects in scene graph generation, I propose a novel training scheme: CMAT. The CMAT is the first model to formulate scene graph generation as a multi-agent cooperative decision-making problem. Based on this formulation, the model can directly utilize the whole scene graph quality as its training objective. Meanwhile, I propose a counterfactual baseline model, which can derive the local contribution of each object category prediction.

3) Based on the existing spatial attention mechanism, I propose a channel-wise attention mechanism. Meanwhile, I take full advantage of the three dimensions (spatial, channel, and multi-layer) of CNN features, and proposes a novel Spatial and Channel-wise Convolutional Networks (SCA-CNN). For image captioning, the SCA-CNN can not only improve the quality of the sentences, but also provide a better understanding of where (ie, spatial) and what (ie, channel-wise) the attention looks like in a CNN that evolves during sentence generation.

4) According to the weaknesses of two mainstream frameworks (top-down models and sparse bottom-up models) for query-based video localization, I propose a novel dense bottom-up model: GDP. Specifically, the GDP disentangles the boundary prediction problem into two sub-problems: relatedness prediction and boundary regression. Meanwhile, GDP contains a Graph Feature Pyramid Network layer to boost the feature from the backbone network. The proposed GDP model can consistently improve the localization accuracy across different query types.

5) Current Visual Question Answering (VQA) models always overlook two indispensable characteristics of an ideal VQA model: visual-explainable and question-sensitive. In this thesis, I propose a model-agnostic Counterfactual Samples Synthesizing (CSS) mechanism. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (ie, the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves the visual-explainable and question-sensitive abilities of VQA models.


\englishkeywords{Complex Visual Scene Understanding, Zero-Shot Visual Recognition, Scene Graph Generation, Image Captioning, Query-based Video Localization, Visual Question Answering}
\end{englishabstract}
