\chapter{基于属性保持的零样本物体分类方法}


零样本物体分类旨在对训练过程中从未见过类别的图像进行分类。为了解决零样本物体分类问题，本章我们提出了一个全新的学习框架：属性保持的对抗网络学习（Semantics-Preserving Adversarial Embedding Networks, SP-AEN）。SP-AEN主要解决了一个目前主流零样本分类框架（基于嵌入映射的模型）中不可避免的问题：语义损失。语义损失是指在训练集的训练过程中，模型往往容易“丢失”一些对训练类别来说区分性不大的属性，但是这些属性可能对测试集（包含不可知的未见类别的图像）非常有用。具体来说，SP-AEN通过引入一个新的映射函数，从而将两个冲突的任务：分类和重建进行分离，映射到两个子空间中。通过对抗学习，SP-AEN可以让重建子空间中部分属性迁移到分类子空间，实现对未见类别的分类。通过与现有零样本分类方法的对比，SP-AEN不仅仅在分类性能上有大幅提升，同时可以生成非常逼真的图像，表现出非常好的属性保持效果。在通用的四个数据集中：CUB、AWA、SUN和aPY， SP-AEN比现有最好的零样本分类方法~\cite{xian2017zero}在H值上分别提升了12.2\%、9.3\%、4.0\%和3.6\%


\section{问题描述}


零样本物体识别（Zero-Shot Recogintion, ZSR）或零样本学习（Zero-Shot Learning, ZSL）是为了能够对训练过程未见过的新类别图像进行分类识别。目前，关于零样本学习问题的难点，学界的共识是如何将已见类别的知识迁移到未见类别上。尽管到目前为止，已经有非常多的零样本分类方法，这些方法都是依据一些非常简单和直观的机制。例如，虽然“浣熊”这个类别在训练的时候没有见过，但是我们仍然可以识别出浣熊的图像，通过检查浣熊这个类别特有的一些属性特征，像“有条纹的尾巴”~\cite{farhadi2009describing,lampert2009learning,zhang2013attribute,li2010object}、“像狐狸的外观”~\cite{torresani2010efficient,li2010object}、以及“浣熊”这个类别的语义信息~\cite{pennington2014glove,mikolov2013distributed}。这些属性特征通常在训练阶段被建模，然后期待在测试阶段中可以在所有的类别（已见类别和未见类别）中共享。经过数十年的发展，目前的零样本学习框架已经从初始基于属性分类器的模型~\cite{lampert2009learning}发展到基于嵌入映射的模型~\cite{akata2015label,frome2013devise,weston2010large}。这种基于嵌入映射的模型往往简单有效，如图~\ref{ch3:fig:zsl_paradigms}（a）所示，这种模型首先将图像从视觉空间（V）映射到语义空间（S），同时，图像类别的语义特征也在该语义空间中。这样映射之后，零样本学习问题就“退化”成一个最近邻的类别查找问题。

\begin{wrapfigure}{r}{0.6\linewidth}
    \centering
        \includegraphics[width=0.95\linewidth]{chapter3/res/zsl_paradigms.pdf}
    \captionof{figure}{三种典型的零样本学习框架}
    \label{ch3:fig:zsl_paradigms}
\end{wrapfigure}

这种嵌入映射模型的知识迁移能力受限于\textbf{语义损失}的问题。如图~\ref{ch3:fig:zsl_paradigms}所示，模型丢失一些对训练集图像方差较小的属性（即，不同类之间区别小的属性）有利于训练集的分类。然而，由于训练集和测试集之间存在差异，这些“丢失”的属性可能对测试集来说具有较大的区别性，这样就会造成对测试集分类的困难。虽然类别在语义空间是一个单独的“点”，具有丰富的语义信息，但是将所有同类的图像从视觉空间映射到这个点附近，就不可避免地造成部分属性的丢失~\cite{lazaridou2015hubness,fu2015transductive}。


为了尽可能多地减少属性的丢失，一个可能的解决方法是通过图像重建，即先将图像从视觉空间映射到语义空间中，然后将语义空间的特征映射回视觉空间。如果映射后的视觉空间的特征能够重建初始的图像，说明语义空间的特征已经尽可能多地保持了原有的属性，否则将无法重建~\cite{kim2017learning,yi2017dualgan,zhu2017unpaired,he2016dual}。然而，图像重建和图像分类是两个相互冲突的目标：前者希望能够尽可能多地保持图像的细节，而后者希望只关注类别差异性大的特征、忽略不相关的特征。例如，只用“头”或者“躯干”就可以充分地对“人”这个类别进行识别分类，而一些其他的颜色属性，如“红色”或者“白色”就需要忽略。为了进一步展示，如图~\ref{ch3:fig:zsl_paradigms}（b）假设 $E$: $\mathcal{V}\rightarrow \mathcal{S}$和$G$: $\mathcal{S}\rightarrow \mathcal{V}$ 是视觉空间与语义空间中的两个映射函数。对于图像分类，我们希望视觉空间中同一类别的两个图像$x, x'\in \mathcal{V}$在语义空间能够接近$s, s'\in\mathcal{S}$， 即, $E(x) = s \approx s' = E(x')$；对于图像重建，我们希望$G(s)\approx x$和$G(s')\approx x'$，这样就很难满足$s\approx s'$。因此，同时训练这两个目标（分类和重建）对于保持属性的效劳往往有限（SAE~\cite{kodirov2017semantic}）。如图~\ref{ch3:fig:reconstruction_visualization}所示，如果我们想实现好的分类结果，那往往重建会失败。


\begin{figure}[h]
    \centering
        \includegraphics[width=\linewidth]{chapter3/res/reconstruction_visualization.pdf}
    \caption{现有模型SAE~\cite{kodirov2017semantic}和提出模型SP-AEN的图像重建结果对比}
    \label{ch3:fig:reconstruction_visualization}
\end{figure}

为了缓解分类任务和重建任务的冲突，本文提出一个全新的视觉-语义映射的框架：属性保持的对抗网络学习（SP-AEN）。如图~\ref{ch3:fig:zsl_paradigms}（c）所示，我们引入一个新的映射函数$F$: $\mathcal{V}\rightarrow \mathcal{S}$和一个对抗优化目标~\cite{goodfellow2014generative}。映射函数$F$和对抗训练的目的是让判别器$D$无法区分这两个不同的映射分布$E(x)$和$F(x)$。具体来说，这样做有两个好处：（1）\textbf{语义迁移}：虽然对于单独的分类映射函数$E$来说，语义损失是不可避免的。我们通过利用判别器$D$的训练，让分类映射向量$E(x)$和重建映射向量$F(x)$在同一个分布下，实现属性特征的迁移，让$E(x)$尽可能地保持更多的属性。（2）\textbf{分类任务与重建任务分解}：映射网络$F$和$G$实现重建任务，而映射网络$E$实现分类任务。通过将分类任务和重建任务进行分解，之前的严格条件$G(E(x)) \approx x$和$G(E(x')) \approx x'$变成了$G(F(x))\approx x$和$G(F(x'))\approx x'$，同时$F(x)$与$F(x')$在语义空间中也不需要非常接近。如图~\ref{ch3:fig:reconstruction_visualization}所示，我们的映射$G(F(x))$可以重建出较好的输入图像，说明属性特征能够更好地保持。

本文在四个通用的零样本分类数据集中对模型SP-AEN的效果进行验证：CUB~\cite{wah2011caltech}，AWA~\cite{lampert2009learning}, SUN~\cite{patterson2012sun}，和aPY~\cite{farhadi2009describing}。相比于目前最好的零样本分类方法~\cite{xian2017zero}，SP-AEN在评价指标H值（Harmonic Mean Value）上对于上述四个数据集各自提升了12.2\%，9.3\%，4.0\%，和3.6\%。据我们了解，SP-AEN是第一个能够直接重建回原始图像的零样本分类方法。


\section{属性保持的对抗网络学习}

在本节，我们首先介绍零样本分类任务，然后再具体介绍本章提出模型SP-AEN的各个优化目标的细节。

\subsection{零样本分类预备知识}
给定一个训练集$\{x_i, l_i\}$，其中$x_i\in\mathcal{V}$是图像在视觉空间的映射向量，$l_i\in \mathcal{L}_s$是已见类别的类别标签，零样本分类任务的目标是学习一个分类器。这个分类器不仅可以预测已见类别的图像（$\mathcal{L}_s$），也可以预测未见类别的图像（$\mathcal{L}_u$）。按照之前工作的总结~\cite{xian2017zero,lei2015predicting}，几乎所有先进的零样本分类方法都是基于嵌入映射的。这类方法旨在找到一个映射函数$\mathcal{V}\rightarrow \mathcal{S}$，其中所有的类别标签在语义空间$\mathcal{S}$都是编码成$\mathbf{y}_l\in\mathbb{R}^d$。因此，预测类别标签$l^*$时，可以直接通过简单的最近邻查找：
\begin{equation}\label{ch3:eq:eq_1}
l^* = \max_{l\in\mathcal{L}}~\mathbf{y}^T_l E(x)
\end{equation}
特别地，如果$l\in\mathcal{L}_u$，这是\textbf{传统型零样本分类}；如果$l \in\mathcal{L}_s\cup \mathcal{L}_u$，这是\textbf{通用型零样本分类问题}。另外，公式~\eqref{ch3:eq:eq_1}中$E$不一定是线性函数，同样可以利用深度神经网络等非线性函数。


\begin{figure}[tbp]
    \centering
        \includegraphics[width=\linewidth]{chapter3/res/sp_aen.pdf}
    \caption{模型SP-AEN的整体结构流程图}
    \label{ch3:fig:sp_aen}
\end{figure}


\subsection{分类任务优化目标}
由于公式~\eqref{ch3:eq:eq_1}的标签预测本质上是一个排序问题，我们使用排序损失（Ranking Loss）作为分类任务的优化目标~\cite{frome2013devise,weston2010large}，即给定一个训练样本$(x, l)$，我们希望$\mathbf{y}_l$和$E(x)$之间有一个较大的点积相似度，而负样本$(x,l')$有较小的点积相似度，并且正样本的相似度相比负样本的相似度要大于一定的阈值：
\begin{equation}\label{ch3:eq:eq_2}
L_{cls} = \sum\limits_{l\neq l'}~\max\{0, \gamma - \mathbf{y}_l^T E(x) + \mathbf{y}^T_{l'} E(x)\}
\end{equation}
其中，$\gamma >0$是一个超参数（即阈值）。在每次训练迭代过程中，$l'$是从所有错误的标签中任选一个。

分类任务优化目标$L_{cls}$的目的是让所有的同一类图像的语义空间映射向量$E(x)$都接近与其类别标签$\mathbf{y}$在语义空间的映射向量。它造成的语义损失问题将由后续介绍的两个额外的优化目标来解决。


\subsection{重建任务优化目标}
重建任务的目标是学习一个从语义空间到视觉空间的映射函数$G$: $\mathcal{S}\rightarrow \mathcal{V}$，使得将语义映射向量$s\in\mathcal{S}$可以重建输入图像，并且使差别$\|G(s)-x\|$很小。由于在自编码器中重建任务$s = E(x)$与分类任务是相互冲突的，我们引入一个新的视觉空间到语义空间的映射函数$F$，使得$s = F(x)$。另外，不同于SAE~\cite{kodirov2017semantic}利用卷积神经网络~\cite{he2016deep,simonyan2015very}的输入作为视觉空间$\mathcal{V}$的特征，我们直接利用原始的$256\times 256 \times 3$大小的RGB色彩空间来进行图像重建。这样做的主要原因是，卷积神经网络的输出特征在网络的预训练阶段已经存在语义损失问题。


为了最小化重建损失，映射向量$F(x)$会尽可能地保持多的属性，以便于重建回输入图像。我们参考最新的图像生成工作~\cite{johnson2016perceptual,dosovitskiy2016generating,ledig2017photo}，将优化目标定义为：
\begin{equation}\label{ch3:eq:eq_3}
L_{rec} = L_{feat}+\lambda_p L_{pixel}
\end{equation}
其中$L_{feat} = \|\phi\left(G\left(F(x)\right)\right)-\phi(x)\|^2_2$是特征维度上的重建损失函数，帮助图像能够保持细节感知上的相似度。我们使用卷积神经网络AlexNet~\cite{krizhevsky2012imagenet}中$conv5$层来表示$\phi$。$L_{pixel} = \|G(F(x))-x\|^2_2$是像素维度上的重建损失函数，有利于算法的稳定性。


\subsection{对抗学习优化目标}
到目前为止，语义向量$E(x)$和$F(x)$之间没有知识迁移。我们的目标是让$E(x)$从$F(x)$中迁移部分丢失的属性特征。然后，手工直接定义$E(x)$与$F(x)$之间的迁移比较困难。因此，我们借助于对抗学习的思想，来鼓励$E(x)$的分布向$F(x)$的分布靠近，通过“欺骗”判别网络$D$，使$F(x)$的知识向$E(x)$迁移：
\begin{equation}\label{ch3:eq:eq_4}
L_{adv} = \mathbb{E}_{x}\left( \log D(F(x)) \right) + \mathbb{E}_{x'}\left( \log \left[1-D(E(x'))\right] \right)
\end{equation}
其中网络$E$为了减少优化目标$L_{adv}$，而网络$D$希望增大优化目标$L_{adv}$，即：$E^* = \arg\min_E \max_D L_{adv}$。

目前，许多研究工作都发现目标函数$L_{adv}$进行优化容易陷入“塌陷问题”~\cite{arjovsky2017wasserstein}。在我们这个任务中，如果两个同类别的图像$x$和$x'$非常相似，容易导致$\|F(x)-E(x')\|\approx 0$，引起塌陷问题。为了避免塌陷问题，我们参考WGAN的策略~\cite{arjovsky2017wasserstein}，极大地增加了模型训练的稳定性。


\subsection{总体优化目标}
将之前提到的分类任务优化目标、重建任务优化目标、以及对抗学习优化目标合在一起，得到最终整体的优化目标：
\begin{equation}\label{ch3:eq:eq_5}
\begin{split}
L (E, F, G, D) = L_{cls}(E) &+ \alpha L_{rec} (E, F, G) \\
&+ \beta L_{adv}(E, F, G, D)
\end{split}
\end{equation}
其中，$\alpha$和$\beta$是参数用于权衡不同的训练目标。最终的目标是得到：
\begin{equation}\label{ch3:eq:eq_6}
E^* = \arg\min\limits_{E, F, G}\max\limits_{D} L(E, F, G, D)
\end{equation}

如图~\ref{ch3:fig:sp_aen}所示，网络$F$是编码网络，网络$G$是解码网络，重建映射向量$F(x)$可以看成是瓶颈层，来约束分类映射向量$E(x)$。另外，SP-AEN可以转换成其他条件下的零样本分类问题，例如半监督条件下，我们只需要给$F(x)$增加一个额外的对抗学习优化目标来近似先验的映射空间。



\section{实验设置与性能分析}
\subsection{零样本物体分类数据集}
\textbf{CUB}~\cite{wah2011caltech}：全称是Caltech-UCSD-Birds 200-2011数据集。它是一个细粒度鸟类别分类数据集，总共包含11788张来自200个细粒度类别的鸟图像，并且每张图像有312个语义属性标注。其中训练集包含150个已见类别的7057张图像，测试集包含150个已见类别的1764张图像和50个未见类别的2967张图像。

\textbf{SUN}~\cite{patterson2012sun}：全称是SUN attribute数据集。它是一个细粒度场景分类数据集，总共包含14340张来自717个场景类别的场景图像，并且每张图像有102个语义属性标注。其中训练集包含645个已见类别的10320张图像，测试集包含645个已见类别的2580张图像和72个未见类别的1440张图像。

\textbf{AWA}~\cite{lampert2009learning}：全称是Animals with Attributes数据集。它是一个动物类别分类数据集，总共包含30475张来自50个类别的动物图像，并且每张图像有85个语义属性标注。其中训练集包含40个已见类别的23527张图像，测试集包含40个已见类别的5882张图像和10个未见类别的7913张图像。由于原始AWA数据集图像版权的问题，我们这里的AWA数据集实际上使用的是AWA2~\cite{xian2017zero}.

\textbf{aPY}~\cite{farhadi2009describing}：全称是Attribute Pascal and Yahoo数据集。它是一个通用的物体分类数据集，总共包含12051张来自32个类别，并且每张图像有64个语义属性标注。其中训练集包含20个已见类别5932张图像，测试集包含20个已见类别1483张图像和12个未见类别的7924张图像。

为了公平地和其他模型进行比较，我们使用Xian等人~\cite{xian2017zero}提供的类别嵌入映射向量，其中每个嵌入映射向量都经过$l_2$范数进行归一化。


\subsection{实验设定与零样本物体分类评价指标}
\noindent{\kaishu{实验设定}}：
为了评估模型对零样本物体分类的结果，我们采用三种实验设定：
\begin{asparaenum}
\item U$\to$U: 测试图像的类别和可以预测的类别都只是未见类别；

\item S$\to$T: 测试图像的类别是未见类别，但是可以预测的类别是未见类别和已见类别的总和；

\item U$\to$T: 测试图像的类别和可以预测的类别都是未见类别和已见类别的总和。
\end{asparaenum}
通常，U$\to$U被称为传统型零样本分类，而U$\to$T被称为通用型零样本分类。

\noindent{\kaishu{评价指标}}:
我们参考现有的文献~\cite{xian2017zero}，常用的每类平均准确率作为评价指标。对于通用型零样本分类，我们另外使用常用的$H$作为主要的评价指标，其中$H$是已见类别$L_s$的准确率（$Acc_{S\rightarrow T}$）和未见类别$L_u$的准确率（$Acc_{U\rightarrow T}$）的调和平均数：
\begin{equation} \label{ch3:eq:eq_7}
H = 2\times Acc_{S\rightarrow T}\times Acc_{U\rightarrow T} /(Acc_{S\rightarrow T}+Acc_{U\rightarrow T})
\end{equation}

\subsection{网络模型与参数设置}
\noindent{\kaishu{网络结构}}：整个网络结构都是端到端地直接进行训练。其中映射网络$E$是基于ResNet-101~\cite{he2016deep}，输入图像的大小是$224\times224\times3$。映射网络$F$是基于AlexNet~\cite{krizhevsky2012imagenet}，然后附加上两层额外的全连接层。重建网络$G$采用类似于生成器~\cite{dosovitskiy2016generating}的结构，通过五个连续的反卷积和非线性操作（leaky ReLU）将向量特征转换成三维卷积特征。


\noindent{\kaishu{参数设置}}：对于本章所有的实验，训练图像都将短边放缩到256个像素。参照AlexNet~\cite{krizhevsky2012imagenet}，我们采用了增大十倍训练图像的数据增强方式。为了提升训练速度，映射网络$E$中ResNet-101部分参数始终保持固定，映射网络$F$的参数初始化采用预训练好的AlexNet的参数，重建网络$G$的参数初始化用预训练好的生成器~\cite{dosovitskiy2016generating}。剩余的所有参数都是用MSRA的随机初始化~\cite{he2015delving}。初始的学习率设置为$1e^{-4}$，然后当loss不下降时，学习率降低10倍。


\subsection{零样本物体分类的性能对比}
本节将本章方法与目前最先进的零样本物体分类方法进行对比。这些方法主要可以分类：（1）基于嵌入映射的模型：DeViSE~\cite{frome2013devise}、ALE~\cite{akata2015label}、SJE~\cite{akata2015evaluation}、ESZSL~\cite{romera2015embarrassingly}、LTM\cite{xian2016latent}、CMT/CMT$^*$~\cite{socher2013zero}和SAE~\cite{kodirov2017semantic}。这类方法和SP-AEN一样，将图像从视觉空间映射到语义空间。据我们了解，SAE是现有的唯一一个利用信号重建来解决语义损失的模型。（2）基于属性的模型：DAP~\cite{lampert2009learning}、IAP~\cite{lampert2009learning}、SSE~\cite{zhang2015zero}、CSE~\cite{norouzi2014zero}和SYNC~\cite{changpinyo2016synthesized}。这类方法只适用于有属性标注的情况。

%%%%%%%%%%%%%%%%%%%%%%%%%% SOTA %%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp]
\centering
\scalebox{0.7}{
\begin{tabular}{|c | c | c c c c c c c c c c c c c|}
\hline
Dataset &  & DAP & IAP & SSE & CSE & SYNC & CMT & LTM & DeViSE & ALE & SJE & ESZSL & SAE & SP-AEN \\
\hline
\multirow{4}{*}{SUN} & $Acc_{U\to U}$ & 39.9 & 19.4 & 51.5 & 38.8 & 56.3 & 39.9 & 55.3 & 56.5 & 58.1 & 53.7 & 54.5 & 40.3 & \textbf{59.2} \\
& $Acc_{U\to T}$ & 4.2 & 1.0 & 2.1 & 6.8 & 7.9 & 8.1 & 14.7 & 16.9 & 21.8 & 14.7 & 11.0 & 8.8 & \textbf{24.9} \\
& $Acc_{S\to T}$ & 25.1 & 37.8 & 36.4 & \textbf{39.9} & 43.3 & 21.8  & 28.8 & 27.4 & 33.1 & 30.5 & 27.9 & 18.0 & 38.6 \\
& H & 7.2 & 1.8 & 4.0 & 11.6 & 13.4 & 11.8 & 19.5 & 20.9 & 26.3 & 19.8 & 15.8 & 11.8 & \textbf{30.3} \\
\hline
\multirow{4}{*}{CUB} & $Acc_{U\to U}$ & 40.0 & 24.0 & 43.9 & 34.3 & \textbf{55.6} & 34.6 & 49.3 & 52.0 & 54.9 & 53.9 & 53.9 & 33.3 & 55.4 \\
& $Acc_{U\to T}$ & 1.7 & 0.2 & 8.5 & 1.6 & 11.5 & 7.2 & 15.2 & 23.8 & 23.7 & 23.5 & 12.6 & 7.8 & \textbf{34.7} \\
& $Acc_{S\to T}$ & 67.9 & \textbf{72.8} & 46.9 & 72.2 & 70.9 & 49.8 & 57.3 & 53.0 & 62.8 & 59.2 & 63.8 & 54.0 & 70.6 \\
& H & 3.3 & 0.4 & 14.4 & 3.1 & 19.8 & 12.6 & 24.0 & 32.8 & 34.4 & 33.6 & 21.0 & 13.6 & \textbf{46.6} \\
\hline
\multirow{4}{*}{AWA} & $Acc_{U\to U}$ & 46.1 & 35.9 & 61.0 & 44.5 & 46.6 & 37.9 & 55.8 & 59.7 & \textbf{62.5} & 61.9 & 58.6 & 54.1 & 58.5 \\
& $Acc_{U\to T}$ & 0.0 & 0.9 & 8.1 & 0.5 & 10.0 & 0.5 & 11.5 & 17.1 & 14.0 & 8.0 & 5.9 & 1.1 & \textbf{23.3} \\
& $Acc_{S\to T}$ & 84.7 & 87.6 & 82.5 & 90.6 & 90.5 & 90.0 & 77.3 & 74.7 & 81.8 & 73.9 & 77.8 & 82.2 & \textbf{90.9} \\
& H & 0.0 & 1.8 & 14.8 & 1.0 & 18.0 & 1.0 & 20.0 & 27.8 & 23.9 & 14.4 & 11.0 & 2.2 & \textbf{37.1} \\
\hline
\multirow{4}{*}{aPY} & $Acc_{U\to U}$ & 33.8 & 36.6 & 34.0 & 26.9 & 23.9 & 28.0 & 35.2 & \textbf{39.8} & 39.7 & 32.9 & 38.3 & 8.3 & 24.1 \\
& $Acc_{U\to T}$ & 4.8 & 5.7 & 0.2 & 0.0 & 7.4 & 1.4 & 0.1 & 4.9 & 4.6 & 3.7 & 2.4 & 0.4 & \textbf{13.7} \\
& $Acc_{S\to T}$ & 78.3 & 65.6 & 78.9 & \textbf{91.2} & 66.3 & 85.2 & 73.0 & 76.9 & 73.7 & 55.7 & 70.1 & 80.9 & 63.4 \\
& H & 9.0 & 10.4 & 0.4 & 0.0 & 13.3 & 2.8 & 0.2 & 9.2 & 8.7 & 6.9 & 4.6 & 0.9 & \textbf{22.6} \\
\hline
\end{tabular}
}
\caption{不同零样本物体分类方法在4个数据集上的性能对比}
\label{ch3:tab:sota}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{定量性能对比}：表~\ref{ch3:tab:sota}总结了不同的零样本分类方法在四个数据集（SUN、CUB、AWA、aPY）和三种不同实验设定下（U$\to$U、U$\to$T、S$\to$T）的性能对比。从表~\ref{ch3:tab:sota}我们能有两个发现：（1）SP-AEN在通用型零样本分类问题能够显著提升性能，如：在$Acc_{U\to T}$和H值两个指标下，SP-AEN能比目前最好的模型提升4\%到12\%。当数据集中训练集和测试集所有属性方差的余弦相似度越大时\footnote{数据集SUN、CUB、AWA、aPY中，训练集和测试集所有属性方差的余弦相似度分别为0.9851、0.9575、0.7459、0.5847。}，提升更加明显，这也表明SP-AEN可以有效地缓解语义损失问题。（2）在传统型零样本分类的设定下（U$\to$U），在绝大多数的情况下，SP-AEN可以得到最佳的性能。在U$\to$U设定下，图像类别的搜索空间仅限于未见类别，然而，语义损失问题可能导致未见类别的图像与已见类别非常接近，造成一定的预测错误。

\subsection{零样本物体分类方法分析}

\begin{wrapfigure}{r}{0.55\linewidth}
    \centering
        \includegraphics[width=0.95\linewidth]{chapter3/res/conflict.pdf}
    \captionof{figure}{三种图像重建网络框架}
    \label{ch3:fig:conflict}
\end{wrapfigure}

\textbf{分类任务与重建任务的冲突}：为了验证本章提出模型SP-AEN的设计动机：分类任务和重建任务时相互冲突的。如图~\ref{ch3:fig:conflict}所示，我们设计了三种可能的重建网络框架，可以实现SP-AEN中语义空间到视觉空间的重建：（1）\textbf{DirectMap}：对于输入图像，我们使用网络$E$将图像从视觉空间映射到语义空间，得到语义嵌入向量，然后使用网络$G$将语义嵌入向量映射回视觉空间。在该网络中，我们固定$E$的参数，只训练网络$G$的参数。DirectMap可以衡量初始的语义嵌入向量包含多少语义信息。（2）\textbf{SAE}~\cite{kodirov2017semantic}：我们采用与SAE模型相同的框架，用重建网络$G$作为解码网络，分类网络$E$作为编码网络，其中的瓶颈层用来分类任务。在训练阶段，我们同时训练网络$E$和$G$的参数。（3）\textbf{SplitBranch}：我们将网络$E$的输出分别输入到两个不同的支路中，其中一条支路用来分类。然后两条主路再连接到一起，合并的语义特征输入到网络$G$中进行重建。


\begin{table}[htbp]
\centering
\begin{tabular}{|c c| c| c| c|}
\hline
Method & \textbf{SUN} & \textbf{CUB} & \textbf{AWA} & \textbf{aPY} \\
\hline
 DirectMap & 0.079 & 0.069 & 0.075 & 0.085 \\
 SAE & 0.285 & 0.281 & 0.259 &  0.275\\
SplitBranch & 0.070 & 0.058  & 0.059 & 0.076 \\
SP-AEN & \textbf{0.053}  & \textbf{0.040}& \textbf{0.047} & \textbf{0.055} \\
\hline
\end{tabular}
\caption{不同重建网络下重建图像与输入图像之间的平均像素差平方}
\label{ch3:tab:conflict_quantitative}
\end{table}


\begin{figure}[htbp]
    \centering
    \scalebox{0.9}{
        \includegraphics[width=\linewidth]{chapter3/res/conflict_visualization.pdf}    
    }
    \caption{不同图像重建网络框架在数据集CUB、SUN、AWA、aPY上的重建结果}
    \label{ch3:fig:conflict_visualization}
\end{figure}

图~\ref{ch3:fig:conflict_visualization}和表~\ref{ch3:tab:conflict_quantitative}分别表示四个数据集中测试集未见类别图像的重建图像和重建差异。从实验结果中，我们可以有以下结论：（1）在数据集CUB和SUN中，DirectMap重建的图像和SP-AEN非常接近，都具有较好的重建结果。然而，在数据集AWA和aPY中，DirectMap的重建效果有明显下降。同样地，这是由于在数据集CUB和SUN中，训练集和测试集所有属性方差的余弦相似度大于AWA和aPY。（2）如果像SAE模型一样同时训练分类网络E和重建网络G，所有的样本都重建失败。如果像SplitBranch模型一样同时训练，我们可以看到重建效果得到显著的提升，十分接近SP-AEN。但是，我们发现在SplitBranch中，分类分支合并时的权重几乎为零。这一方面说明分类的映射向量基本对重建任务没有贡献，另一方面也引导我们借助对抗学习来实现语义迁移和高质量图像的重建。

\textbf{网络D和网络G的作用}：由于可见类别标签的分数常常大于未见类别标签的分数，通常使用“校准规则”来解决这个问题。具体来说，就是对已见类别的分数减掉一个偏置，然后再和未见类别一起进行排序比较：

\begin{equation}\label{ch3:eq:eq_8}
    l^* = \max_{l\in \mathcal{L}_u \cup \mathcal{L}_s }~\mathbf{y}^T_l E(x) - \gamma \mathbf{1} \left[ l \in \mathcal{L}_s \right]
\end{equation}
其中，指示函数$\mathbf{1} \left[ \cdot \right]$用于判断$l$是否时已见类别，$\gamma \in \mathbb{R}$是校准系数。这个校准规则能够有效地实现对已见类别和未见类别预测之间的权衡。通过改变校准系数$\gamma$，我们可以得到一系列分类准确率（$Acc_{U \to T}$和$Acc_{S \to T}$），并且可以画出已见-未见准确率曲线（Seen-Unseen accuracy Curve, SUC）。已见-未见准确率曲线下区域面积（Area Under Seen-Unseen accuracy Curve, AUSUC）也是通用型零样本分类问题中一个常用评价指标，用来评估$Acc_{U \to T}$和$Acc_{S \to T}$之间的权衡。

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{chapter3/res/ausuc.pdf}
    \caption{在数据集SUN、CUB、AWA和aPY中的已见-未见准确率曲线下区域面积~\cite{chao2016empirical}}
\label{ch3:fig:ausuc}
\end{figure}


xxxx


\section{本章小结}

本章提出了一个全新的零样本分类框架：属性保持的对抗网络学习（SP-AEN），用于解决目前零样本分类模型中普遍存在的语义损失的问题。SP-AEN主要通过两个设计来解决语义损失问题：（1）引入一个独立的重建网络，同时重建网络不直接影响分类网络的优化目标。（2）通过对抗学习，实现重建语义向量和分类语义向量之间的知识迁移。我们通过大量的实验对比在四个标准数据集上验证了SP-AEN的性能。

