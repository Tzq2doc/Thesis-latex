\chapter{相关研究综述}

本章将就零样本物体分类、图像场景图生成、图像描述生成、视频片段检索和图像视觉问答几方面的相关工作和本文的关系进行综述。

本文提出的算法和其相关工作的具体细节和对比将在之后各章节中展示。


\section{零样本物体分类}

\subsection{零样本学习}
零样本物体识别的主流方法是借助类别属性对物体进行零样本学习~\cite{farhadi2009describing,lampert2009learning,romera2015embarrassingly,norouzi2014zero,demirel2017attributes2classname,jiang2017learning}：这类方法通常将类别属性看成是一个共同语义空间的中间特征，从而实现对不同类别之间的语义迁移。为了扩大零样本迁移能力，目前的大多数方法都是基于嵌入映射的~\cite{frome2013devise,akata2015label,akata2015evaluation,romera2015embarrassingly,xian2016latent,socher2013zero,kodirov2017semantic,li2017zero}。它们通过将图像特征从视觉空间直接映射到语义空间，然后在语义空间与类别属性的嵌入向量进行相似度对比。语义空间中的嵌入向量既可以来自于单词~\cite{mikolov2013distributed,pennington2014glove}，也可以来自于语句~\cite{lei2015predicting,elhoseiny2013write}。我们提出的零样本算法模型算法模型SP-AEN也是一种基于嵌入映射的方法。然而，据我们了解，我们是第一个零样本分类算法，可以用语义空间的嵌入向量来重建原始图像。另外，与零样本学习非常接近的两个任务是少样本学习~\cite{hariharan2017low}和领域自适应~\cite{motiian2017unified,panareda2017open}。这两个任务在训练阶段都能有少量测试类别的图像，相反，零样本学习在训练阶段没有任何测试类别的信息。


\subsection{域偏移问题}
语义损失在其他的文献中也常常被称为域偏移问题~\cite{fu2015transductive,saenko2010adapting}。
域偏移问题是所有物体识别任务中一个非常普遍的问题。只要训练集数据和测试集数据分布不同，就存在域偏移问题。目前，需要的研究工作都发现重建原始信号可以缓解域偏移问题~\cite{kim2017learning}。在本文，我们发现同时进行重建任务和分类任务~\cite{kodirov2017semantic}对于保持语义特征并不是很有效。另一种缓解语义损失的办法是增加一个单独的属性分类器~\cite{morgado2017semantically}，但是这个方法需要额外的属性标注信息。


\subsection{对抗生成网络}
对抗生成网络~\cite{goodfellow2014generative}主要是训练一个生成器，使得生成器生成的样本和真实数据非常“相似”，可以“骗过”判别器。理论上，这种对抗的训练过程可以让生成器生成的样本分布和真实的样本分布完全相同。我们的零样本分类模型SP-AEN将这种对抗思想运用到特征层面上~\cite{odena2017conditional,tzeng2017adversarial,makhzani2015adversarial,shrivastava2017learning}。与此同时，少数零样本分类的模型借助于对抗生成网络来生成更多的训练样本，从而将零样本分类问题转化为普通的分类问题~\cite{mishra2018generative}。但是这类方法违背了零样本分类问题中的一个基本假设：训练阶段中测试集的类别信息是无法知道的。


\section{图像场景图生成}

\subsection{场景图生成}
在第一个视觉关系检测工作~\cite{lu2016visual}以及大规模图像场景图数据集~\cite{krishna2017visual}出现之后，图像场景图生成任务逐渐成为一个新的研究热点。在早期阶段，许多场景图生成任务将物体和视觉关系拆分成两个独立的任务~\cite{lu2016visual,zhang2017visual,zhuang2017towards,zhu2018deep,zhang2017relationship}。但是这些方法忽略了场景中所有物体与视觉关系之间内在联系。为了利用所有物体与视觉关系的联系，最近的一些场景图生成算法都利用信息传递机制（Message Passing）~\cite{xu2017scene,dai2017detecting,li2017scene,li2018factorizable,yin2018zoom,jae2018tensorize,yang2018graph,tang2019learning,gu2019scene,qi2019attentive,wang2019exploring}。但是由于这些方法仍然是利用交叉熵作为模型的优化目标，在整个图结构层面不具备整体一致性。与现有方法不同，本文提出的CMAT模型可以同时满足场景图生成优化目标的两个要求：整体一致性和局部敏感性。


\subsection{多智能体策略梯度}

策略梯度是一种对不可导优化目标进行优化的方法，已经广泛应用在多个场景理解的任务中，如：图像描述生成~\cite{ranzato2016sequence,ren2017deep,liu2017improved,rennie2017self,zhang2017actor,liu2018context}，图像视觉问答~\cite{hu2017learning,johnson2017inferring}，图文匹配~\cite{chen2017query,yu2017joint}，视觉对话~\cite{das2017learning}，和物体检测~\cite{caicedo2015active,mathe2016reinforcement,jie2016tree}。Liang等人~\cite{liang2017deep}将图像场景图生成任务看成一个单智能体决策过程。相反，本文将图像场景图生成任务看成一个多智能体协同决策过程，并且将优化目标设成最终的图像场景图生成质量，利用多智能体策略梯度进行优化。与此同时，与目前许多现有的多智能体工作不同~\cite{foerster2016learning,omidshafiei2017deep}，本文提出的CMAT模型里智能体的数量（64个物体）与动作空间（151个物体类别）都非常大。



\section{图像描述生成}

\subsection{编码-解码框架}

图像描述生成任务（Image Captioning）通常被认为是一种多模态的“翻译”任务，即模型将视觉图像“翻译”成自然语言描述。由于端到端编码-解码框架在机器翻译任务（Neural Machine Translation, NMT）~\cite{sutskever2014sequence}的成功，许多的图像描述生成模型也开始借鉴使用编码-解码框架。最早的基于编码-解码框架的图像描述生成模型是NIC~\cite{vinyals2015show}。NIC用一个卷积神经网络将原始输入图像编码成一个固定的视觉特征向量，然后将该视觉特征向量作为一个递归神经网络的初始时刻的输入，利用递归神经网络逐步将视觉特征向量解码成描述语句。类似地，Karpathy等人~\cite{karpathy2015deep}将编码的视觉特征向量作为递归神经网络隐含状态的初始化，通过引入一个额外的“START”字符触发递归神经网络对视觉特征进行解码。

由于在大规模图像分类数据集ImageNet预训练的卷积神经网络（如：VGG~\cite{simonyan2015very}、GoogLeNet~\cite{szegedy2015going}、ResNet~\cite{he2016deep}等）通常可以提取较好的图像视觉特征，之后的许多基于编码-解码框架的改进工作主要集中于完善解码过程。例如，Donahue等人~\cite{donahue2015long}和Mao等人~\cite{mao2015deep}提出在递归神经网络迭代的每个时刻都输入视觉特征向量，避免生成句子过长时图像特征的影响逐渐减弱。Wang等人~\cite{wang2016image}提出使用双边递归神经网络作为解码器，避免单向递归神经网络LSTM~\cite{hochreiter1997long}只考虑之前时刻的单词信息。

\subsection{注意力机制}

在解码器生成语句的过程中，可以通过引入注意力机制~\cite{bahdanau2014neural}使得模型在预测每个单词的时候动态地调整视觉特征向量，增强编码器的表达能力。

\textbf{空间注意力机制}：Xu等人~\cite{xu2015show}首次将注意力机制应用于图像描述生成任务中。具体来说，Xu等人在卷积神经网络的最后一层特征图中引入空间注意力机制，让模型在每个时刻动态地关注不同的空间区域，合成新的视觉特征。类似地，Zhu等人将同样的空间注意力机制也运用到图像视觉问答任务~\cite{zhu2016visual7w}。除了在最后一层特征图只使用一次空间注意力加权，Yang等人~\cite{yang2016stacked}和Xu等人~\cite{xu2016ask}提出通过叠加使用多次空间注意力加权来提升模型性能。相比于之前的模型只在卷积神经网络的特征图中使用空间注意力加权，Anderson等人~\cite{anderson2018bottom}和Li等人~\cite{li2016visual}提出先对图像进行目标检测，然后对物体级别特征使用空间注意力机制可以进一步提升模型性能。

\textbf{属性注意力机制}



\textbf{自注意力机制}












\section{视频片段检索}

\subsection{基于语句查询的视频片段检索}
基于语句查询的视频片段检索，是一个典型的多模态问题。目前，主流的方法都是基于自顶向下的框架，这些方法主要关注如何设计更强的多模态融合模型，如基于视频的查询注意力机制~\cite{liu2018attentive}、基于查询的视频注意力机制~\cite{liu2018cross}、查询-视频的协同注意力机制~\cite{chen2018temporally,chen2019localizing,yuan2019find}。据我们了解，绝大多数的模型都是自顶向下或自底向上框架，只有两个例外：RWM~\cite{he2019read}和SM-RL~\cite{wang2019language}。这两个方法都是将视频片段检索问题转换成时序决策问题，然后利用梯度策略进行优化，其中的动作空间为时序窗口的变化或帧的跳变。


\subsection{基于视频查询的视频片段检索}
基于视频查询的视频片段检索的主要困难来自于查询视频和参考视频之间巨大的场景差异，包括背景、物体、视角等不同。目前最好的视频查询的视频片段检索是CGBM~\cite{feng2018video}，它也是基于稀疏型自底向上模型。


\subsection{自上向下框架与自底向上框架}







\section{图像视觉问答}


\subsection{视觉问答模型的文本偏差}


\subsection{视觉问答模型的特性}

