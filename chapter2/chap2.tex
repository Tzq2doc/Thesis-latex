\chapter{相关研究综述}

本章将就零样本物体分类、图像场景图生成、图像描述生成、视频片段检索和视觉问答几方面的相关工作和本文的关系进行综述。

本文提出的算法和其相关工作的具体细节和对比将在之后各章节中展示。


\section{零样本物体分类}

\subsection{传统型零样本物体分类}
零样本物体分类或零样本学习，是指模型在训练集中只对部分图像类别（已见类别）进行训练，而在测试阶段对其他从未见过的类别（未见类别）图像进行分类的任务。零样本物体分类任务的设定是测试阶段给定未见类别的类别语义信息。因此，目前的零样本物体分类方法都是通过利用已见类别和未见类别的类别信息在语义空间中的关联性，实现零样本分类~\cite{farhadi2009describing,lampert2009learning,romera2015embarrassingly,norouzi2014zero,demirel2017attributes2classname,jiang2017learning}。例如：“斑马”类别的语义信息是“有条纹图案的马”。当知道“马”和“斑马”两个类别在语义空间的联系之后，同时训练集中包含大量“马”类别的图像，就可以实现在测试集对“斑马”类别图像进行分类。

早期的零样本物体分类模型通常为二阶段模型~\cite{lampert2013attribute,norouzi2014zero,al2016recovering,jayaraman2014zero,kankuekul2012online}。在第一阶段，这些模型首先预测出图像所包含的属性特征。然后，在第二阶段，模型根据预测的属性特征来推测出物体的类别。为了扩大零样本物体分类模型的迁移能力，目前的大多数方法都是基于嵌入映射的模型~\cite{palatucci2009zero,frome2013devise,akata2015label,akata2015evaluation,romera2015embarrassingly,xian2016latent,socher2013zero,kodirov2017semantic,li2017zero}：在训练集中学习一个图像视觉特征向量和类别语义嵌入向量之间的映射函数，然后将学习到的映射函数迁移到测试集中，以同样的方式对测试集图像进行分类。最早的基于嵌入映射的模型是SOC~\cite{palatucci2009zero}。模型SOC直接将图像视觉特征向量从视觉空间映射到语义空间，然后在语义空间中计算映射后的特征向量与类别语义嵌入向量之间的相似度。之后，模型ALE~\cite{akata2015label}和模型DeViSE~\cite{frome2013devise}开始提出在语义空间中使用排序损失函数（ranking loss）作为模型的优化目标。除了线性地将图像视觉特征向量映射到语义空间，一些模型开始使用非线性的映射函数~\cite{xian2016latent}。例如：模型CMT~\cite{socher2013zero}使用两层神经网络来拟合图像视觉特征向量到语义空间的映射函数。为了进一步提升图像视觉特征向量的表达能力，Ba等人~\cite{lei2015predicting}提出要让图像编码网络和映射函数一起进行端到端训练。

除了将图像特征从视觉空间映射到语义空间中，Zhang等人~\cite{zhang2017learning}认为视觉空间比语义空间具有更强的区分能力，将类别语义嵌入向量映射到视觉空间中，并在视觉空间中计算映射后的语义向量和图像视觉特征向量的相似度，可以更加有效地避免语义丢失的问题。另一方面，部分模型认为可以同时将视觉特征和语义特征都映射到其他的共同空间。例如：SSE~\cite{zhang2015zero}直接将已见类别的线性组合作为映射的共同空间。JLSE~\cite{zhang2016zero}则分别将图像的视觉特征和类别的语义特征分别映射到两个单独的子空间，然后计算两个子空间之间的相似度。

类别在语义空间中的嵌入向量主要来自于属性标注。因为每张图像或者类别的属性信息需要大量的人工标注，所以一些工作开始研究使用其他的辅助信息来得到语义空间的嵌入向量。例如：模型SJE~\cite{akata2015evaluation}使用了四种不同语义嵌入向量（属性、类别的word2vec编码~\cite{mikolov2013distributed}、类别的GloVe编码~\cite{pennington2014glove}和类别的wordnet编码~\cite{miller1995wordnet}）。同样，语义嵌入向量不仅可以来自于标注的单词，也可以来自于标准的描述语句~\cite{reed2016learning,lei2015predicting,elhoseiny2013write}。


\subsection{通用型零样本物体分类}
在传统型零样本物体分类中，假定测试阶段的所有图像都只来自于未见类别，即模型预测类别的选择空间只限于未见类别。这种实验设定大大降低了零样本学习的难度以及在实际场景中的运用价值，因为在实际场景中会同时遇到已见类别和未见类别的图像。为了更加符合真实场景，Scheirer等人~\cite{scheirer2012toward}提出通用型零样本物体分类~\cite{scheirer2012toward}任务：模型在训练阶段只对已见类别图像进行训练，而在测试阶段，图像既可能来自于已见类别，也可能来自于未见类别。对于通用型零样本物体分类，Bendale等人~\cite{bendale2016towards}首次将其转化为异常检测问题，即利用一个网络预先判断图像是否来自未见类别，如来自未见类别，则问题转化为传统型零样本物体分类问题。

因为在训练过程中模型已经见过了大量的已见类别图像而没有见过任何未见类别图像，所以模型在预测类别时更倾向于将图像分类成已见类别。为了权衡未见类别和已见类别之间的预测分数，Chao等人~\cite{chao2016empirical}提出通过对已见类别减去一定的偏置再和未见类别一起比较。

为了更好的评估通用型零样本分类，许多工作提出了不同的评价指标~\cite{chao2016empirical,xian2018zero}来权衡已见类别和未见类别。例如：Chao等人~\cite{chao2016empirical}提出通过不断改变已见类别分数减去的偏置，可以得到一条已见-未见准确率曲线，通过计算已见-未见准确率曲线下方面积可以用来评估已见类别和未见类别之间的权衡。Xian等人~\cite{xian2018zero}提出为了增大稀有类别的影响，计算准确率时应当计算所有类别准确率的平均，而不是所有图像准确率的平均。

\subsection{零样本学习中的域偏移问题}
语义丢失问题，也常常被称为域偏移问题（domain shift）~\cite{fu2015transductive,saenko2010adapting}，是零样本学习、少样本学习~\cite{hariharan2017low}和领域自适应~\cite{motiian2017unified,panareda2017open}
等迁移学习任务中一个非常普遍存在的问题。只要当训练集数据和测试集数据的分布不同时，就存在域偏移问题。

因为在训练过程中，模型都倾向于丢失对训练类别分类区分性较小的语义特征；而由于零样本物体分类中已见类别和未见类别的差异，丢失的语义特征可能对未见类别区分性较大，造成测试集分类准确率下降。目前的研究工作发现重建原始信号可以缓解域偏移问题~\cite{kim2017learning}。在零样本物体分类，Kodirov等人~\cite{kodirov2017semantic}通过同时进行图像分类和语义属性重建来缓解属性丢失问题。在本文，我们通过实验发现利用同一个网络同时进行重建和分类这两个相互冲突的任务对于保持语义特征并不是很有效。为了更加有效地缓解语义丢失问题，我们将图像分类和图像重建两个任务分离出来，然后利用对抗学习来实现语义的迁移。

\subsection{零样本学习和对抗生成网络}
对抗生成网络~\cite{goodfellow2014generative}主要是训练一个生成器，使得生成器生成的样本和真实数据非常“相似”，可以“骗过”判别器。理论上，这种对抗的训练过程可以让生成器生成的样本分布和真实的样本分布完全相同。对于零样本分类分类任务，一些模型开始借助于对抗生成网络来生成更多的训练样本，从而将零样本分类问题转化为普通的全监督分类问题~\cite{mishra2018generative,xian2018feature,xian2019f}。尽管这类方法目前已经取得很好的实验结果，但是它们违背了零样本分类问题的一个基本假设：训练阶段中测试集的类别信息是无法知道的。相反，本文提出的零样本分类模型只是将对抗网络中的对抗思想运用到特征层面上~\cite{odena2017conditional,tzeng2017adversarial,makhzani2015adversarial,shrivastava2017learning}，使得分类特征向量的分布向重建特征向量的分布靠近，进而让分类特征向量尽可能地保持更多的属性特征，减少语义丢失。


\section{图像场景图生成}

\subsection{场景图生成}
图像场景图生成任务或视觉关系检测任务，是指在检测出图像中所有物体的基础上，同时对两两物体间的视觉关系进行分类。随着大规模图像场景图数据集~\cite{krishna2017visual}的出现，图像场景图生成任务逐渐成为视觉场景理解领域一个新的研究热点。目前，绝大多数的场景图生成模型都是将场景图生成任务分为两个步骤：（1）利用预训练好的目标检测器~\cite{lu2016visual,zhuang2017towards,zhang2017visual,dai2017detecting,yang2018shuffle,yu2017visual}或在图像场景图生成数据集上重新微调的目标检测器~\cite{li2017vip,xu2017scene,yin2018zoom,zellers2018neural,zhang2017relationship,zhang2019large}对图像进行物体定位。（2）对于所有检测的物体的类别以及两两物体间的视觉关系类别进行预测。通常这些方法都会直接将第一步目标检测后得到的物体位置，当成模型最终预测的物体位置，而只对物体类别和视觉关系类别进行预测。


在早期阶段，许多场景图生成模型都将物体类别预测和视觉关系类别预测拆分成两个独立的任务~\cite{lu2016visual,zhang2017visual,zhuang2017towards,zhu2018deep,zhang2017relationship}。这样的拆分忽略了场景内所有物体的内在联系。为了充分地利用场景内所有物体间的诱导偏置（inductive bias），一些场景图生成模型开始利用信息传递机制（message passing）~\cite{xu2017scene,dai2017detecting,li2017scene,li2018factorizable,yin2018zoom,jae2018tensorize,yang2018graph,tang2019learning,gu2019scene,qi2019attentive,wang2019exploring}来融合周围的信息。例如：Xu等人~\cite{xu2017scene}将物体和视觉关系分别看成两个独立的递归神经网络的节点，在递归神经网络的每个迭代过程中两个节点（物体节点和视觉关系节点）互相传递信息，每个物体节点和视觉关系节点通过接收传递的信息来更新自身节点的内部特征表达，从而充分利用图像的全局信息。Li等人~\cite{li2017scene}通过引入图像密集描述生成任务（dense captioning），构建三种不同的节点（物体、视觉关系以及密集描述框），然后利用同样的信息传递机制来编码全局信息，更新节点的特征表达。Yang等人~\cite{yang2018graph}只将物体看成节点，然后利用图卷积（graph convolution）来融合每个物体周围的视觉信息。Zellers等人~\cite{zellers2018neural}使用递归神经网络直接将所有的物体节点串接在一起，来融合所有物体特征。与此同时，Zellers等人~\cite{zellers2018neural}发现目前数据集中标注的视觉关系包含大量的文本偏置，即利用物体类别和视觉关系类别之间的统计频率就可以得到较高的视觉关系预测准确率。目前的场景图生成模型开始将这种训练集图像中视觉关系的统计频率作为视觉关系的一种先验知识。

目前的图像场景图生成方法都在关注于如何更好地编码物体周围的视觉信息，并且它们都是直接利用所有物体和视觉关系分类的交叉熵之和作为模型的优化目标。在本文，我们分析了当前优化目标的缺陷，并首次将图像场景图生成任务转化成一个多智能体协同决策问题，并直接将最终的场景图生成质量作为优化目标。

\subsection{多智能体策略梯度}

由于本文直接使用场景图生成的评价指标作为模型的优化目标，我们借助多智能体策略梯度的算法对模型进行参数优化。策略梯度是一种强化学习模型中常用的优化策略，同样也是一种对不可导优化目标进行优化的方法。在视觉场景理解任务中，策略梯度已经广泛应用于多种任务中，如：图像描述生成~\cite{ranzato2016sequence,ren2017deep,liu2017improved,rennie2017self,zhang2017actor,liu2018context}，图像视觉问答~\cite{hu2017learning,johnson2017inferring}，图文匹配~\cite{chen2017query,yu2017joint}，视觉对话~\cite{das2017learning}，和目标检测~\cite{caicedo2015active,mathe2016reinforcement,jie2016tree}等。

对于视觉场景图生成问题，目前只有Liang等人~\cite{liang2017deep}将图像场景图生成任务看成一个单智能体的序列决策问题。它首先根据文本的先验信息构建一个有向的语义知识图谱，然后根据语义知识图谱在序列决策的过程中分别选取下一步需要预测的物体和视觉关系，逐步构建视觉场景图。相反，本文将图像场景图生成任务看成一个多智能体协同决策过程，每个物体看成一个单独的智能体。通过把场景图生成转化成多智能体协同决策过程，我们可以直接使用整体场景图生成质量作为优化目标。值得注意的是，与目前许多现有的多智能体协同决策工作不同~\cite{foerster2016learning,omidshafiei2017deep}，在本文的模型设计中，单个图像中智能体的数量（64个检测物体）与动作空间范围（151个物体类别）都非常大。


\section{图像描述生成}

\subsection{编码器-解码器框架}

图像描述生成任务，即模型生成一个描述语句来概括图像场景内容。它通常被看成是一种多模态的“翻译”任务，即模型将视觉图像“翻译”成自然语言描述。随着端到端编码器-解码器框架在机器翻译任务（Neural Machine Translation, NMT）~\cite{sutskever2014sequence}的成功，许多的图像描述生成模型也开始借鉴使用编码器-解码器框架。最早的基于编码器-解码器框架的图像描述生成模型是NIC~\cite{vinyals2015show}。NIC用一个卷积神经网络（编码器）将原始输入图像编码成一个固定的视觉特征向量，然后将该视觉特征向量作为一个递归神经网络的初始时刻的输入，利用递归神经网络（解码器）逐步将视觉特征向量解码成描述语句。类似地，Karpathy等人~\cite{karpathy2015deep}将编码的视觉特征向量作为递归神经网络隐含状态的初始化，并通过引入一个额外的“START”字符来触发递归神经网络对视觉特征进行解码。

由于在大规模图像分类数据集ImageNet预训练的卷积神经网络（如：VGG~\cite{simonyan2015very}、GoogLeNet~\cite{szegedy2015going}、ResNet~\cite{he2016deep}等）已经可以提取较好的图像视觉特征，之后的许多基于编码器-解码器框架的改进工作主要集中于完善解码过程。例如，Donahue等人~\cite{donahue2015long}和Mao等人~\cite{mao2015deep}提出在递归神经网络迭代的每个时刻都输入视觉特征向量，避免当生成描述语句过长时图像特征的影响逐渐减弱。Wang等人~\cite{wang2016image}提出使用双边递归神经网络（Bi-LSTM）作为解码器，可以避免单向递归神经网络~\cite{hochreiter1997long}只考虑之前时刻的单词信息的缺陷。

\subsection{注意力机制}

在编码器-解码器框架中，可以通过在解码生成语句的过程中引入注意力机制~\cite{bahdanau2014neural}，来动态地对每个时刻的视觉特征向量进行调整，增强图像视觉特征的表达能力。

\textbf{\kaishu{空间注意力机制}}：Xu等人~\cite{xu2015show}首次将注意力机制应用于图像描述生成任务中。具体来说，Xu等人在卷积神经网络的最后一层特征图中引入空间注意力，让模型在每个时刻动态地对视觉特征进行空间维度上加权，得到新的视觉特征向量。类似地，Zhu等人将同样的空间注意力机制也运用到其他视觉场景理解任务中，如：图像视觉问答任务~\cite{zhu2016visual7w}。除了只使用一次空间注意力加权，Yang等人~\cite{yang2016stacked}和Xu等人~\cite{xu2016ask}同时提出可以通过叠加使用多次空间注意力加权来进一步提升模型性能。相比于之前的模型只在卷积神经网络的特征图中使用空间注意力加权，Anderson等人~\cite{anderson2018bottom}和Li等人~\cite{li2016visual}提出可以先对图像进行目标检测，然后对物体级别特征使用注意力机制。相比于之前直接对卷积神经网络特征图进行注意力加权，物体级别特征能够更好地编码完整的物体信息。

\textbf{\kaishu{语义注意力机制}}：除了使用图像的视觉特征来引导解码器的描述语句生成，一些模型开始引入外部的语义信息来增加解码过程~\cite{wu2016what,you2016image,pan2017video,yao2017boosting}。其中，You等人~\cite{you2016image}提出先检测图像中包含的属性信息，然后对所有的属性信息使用注意力机制，让模型在生成不同单词的过程中关注相应的属性。Yao等人~\cite{yao2017boosting}不仅引入属性来提升图像的描述语句生成，同时研究如何设计网络结构使得模型在生成描述语句过程中能够更好的利用属性信息。Jia等人~\cite{jia2015guiding}通过将图像和当前生成语句的相关性看成一个全局的语义信息，来引导语句生成。但是，这些模型需要额外地提取语义信息，如属性等。在本文提出的多层空间和通道注意力网络中，每个卷积核本质上可以看成是多个语义检测器~\cite{zeiler2014visualizing}。因此，通道注意力机制也可以看成是一种特定的语义注意力机制。


\textbf{\kaishu{自注意力机制}}：自注意力机制~\cite{vaswani2017attention}是先将特征集合（如：特征图区域特征或物体特征）通过不同的映射矩阵分别映射为查询特征矩阵（query）、键特征矩阵（key）和值特征矩阵（value）。通过查询特征矩阵和键特征矩阵计算相似度得到自注意力权重矩阵，然后利用自注意力权重矩阵对值特征矩阵进行加权，得到加权后的特征向量。基于自注意力机制的图像描述生成模型主要是利用Transformer结构~\cite{vaswani2017attention}：用编码网络对图像进行编码，解码网络对编码特征进行解码。其中编码网络和解码网络中都包含大量的自注意力机制模块。Herdade等人~\cite{herdade2019image}首次基于Transformer结构，并且将图像中物体间的几何位置作为重要的语义信息，帮助生成描述语句。与此同时，Li等人~\cite{li2019entangled}通过引入外部的语义标签来增强图像描述的内容。Huang等人~\cite{huang2019attention}引入额外的注意力机制对加权后的特征进行再一次加权。不同于之前的模型都只基于原始的Transformer结构，Cornia等人~\cite{cornia2020m}通过将编码网络中所有层级的特征都作为每层解码网络的输入，对Transformer结构进行改进来进一步提升描述语句生成质量。


\section{视频片段检索}

\subsection{视频片段检索}

给定一个查询和一段视频序列，视频片段检索任务需要在视频序列中定位出两个视频边界时刻（起始时刻和终止时刻），使得两个边界时刻内的视频片段内容和查询内容一致。基于查询形式的不同，目前的视频片段检索任务可以分为：

\textbf{\kaishu{基于语句查询的视频片段检索}}：
基于语句查询的视频片段检索是一个典型的多模态问题，它需要模型同时对视频和自然语句都进行充分的理解。目前，主流的方法都是基于自顶向下或自底向上框架。这些方法主要关注在如何设计更强的多模态融合模型，例如：基于视频的查询注意力模型~\cite{liu2018attentive}、基于查询的视频注意力模型~\cite{liu2018cross}和查询-视频的协同注意力模型~\cite{chen2018temporally,chen2019localizing,yuan2019find}。除了典型的自顶向下或自底向上模型外，目前还有两个模型（RWM~\cite{he2019read}和SM-RL~\cite{wang2019language}）将视频片段检索问题转换成时序决策问题，然后利用梯度策略对模型进行优化。这些模型通常将时序滑窗的变化或帧的跳变设为智能体的动作空间。

\textbf{\kaishu{基于视频查询的视频片段检索}}：
当查询为视频片段时，尽管模型不需要对多模态特征进行融合，但是查询视频和参考视频之间往往存在巨大的场景差异，包括背景、物体、视角等不同。目前最好的基于视频查询的视频片段检索是CGBM~\cite{feng2018video}，它也是一种典型的稀疏型自底向上模型。


\subsection{自顶向下与自底向上思想}
自顶向下和自底向上思想，是计算机视觉领域中解决问题的两种重要思路。本文将现有的视频片段检索方法都归纳为自顶向下模型和自底向上模型。在其他的视觉场景理解研究中，与本文最为接近的自顶向下与自底向上概念还存在于：

\textbf{\kaishu{目标检测}}：随着目标检测器Faster R-CNN~\cite{ren2015faster}的出现，目前绝大多数的目标检测算法都是基于自顶向下的框架，即在图像特征图的每个位置上预先定义大量的锚框，然后对每个锚框分别进行物体类别分类和坐标位置回归。这类自顶向下的模型拥有和自顶向下的视频片段检索方法相同的缺点（即需要预先定义大量的规则）。随着第一个性能相当的自底向上目标检测器的出现：CornerNet~\cite{law2018cornernet}，自底向上的目标检测模型开始逐渐受到人们的关注，近期也开始出现一些其他的自底向上的目标检测器，如ExtremeNet~\cite{zhou2019bottom}、CenterNet~\cite{zhou2019objects,duan2019centernet}和FCOS~\cite{tian2019fcos}等。

\textbf{\kaishu{注意力机制}}：自底向上的注意力机制在许多的视觉场景理解任务中发挥重要的作用，例如：图像描述生成~\cite{xu2015show,chen2017sca}，视觉问答~\cite{xu2016ask,ye2017video}等。最近，Anderson等人~\cite{anderson2018bottom}提出通过融合自底向上注意力和自顶向下注意力，进一步提升了多个视觉场景理解任务的模型性能。

因此，如何更好地融合或者权衡自顶向下与自底向上两种思路，也将是未来的一个重要的研究方向。


\section{图像视觉问答}

\subsection{视觉问答模型}

典型的图像视觉问答模型通常包含图像编码模块、文本编码模块、多模态融合模块和答案分类器。图像编码模块和文本编码模块分别对图像和问题进行编码，然后利用多模态融合模块将两个模态的编码向量进行融合，最后利用分类器直接基于融合后的多模态特征进行答案预测~\cite{zhou2015simple,antol2015vqa,kim2016multimodal}。由于每个不同的问题往往只涉及到图像的部分区域，之前的模型利用图像编码模块对图像提取全局特征往往容易忽略重要的细节。为了避免上述问题，一些视觉问答模型开始引入注意力机制，针对不同的问题来动态地提取视觉特征。Chen等人~\cite{chen2016abc}将问题的特征向量映射成一个卷积核，然后对图像的特征图进行卷积。Yang等人~\cite{yang2016stacked}通过迭代多个空间注意力机制，逐渐优化空间注意力的权重。Fukui等人~\cite{fukui2016multimodal}、Kim等人~\cite{kim2017hadamard,kim2018bilinear}、Yu等人~\cite{yu2017multi,yu2018beyond}、Ben等人~\cite{ben2017mutan,ben2019block}分别提出了不同的多模态双线性融合方式来融合问题的文本特征和不同图像区域的视觉特征。Anderson等人~\cite{anderson2018bottom}提出融合自底向上注意力和自顶向下注意力，通过对物体框进行注意力机制，而不是直接对特征图进行注意力加权。

除了对图像的视觉场景内容进行理解外，视觉问答同时还需要对问题进行理解。因此，对问题使用注意力机制同样可以让模型更加关注重要的文本信息。Lu等人~\cite{lu2017hierarchical}提出协同注意力机制（co-attention mechanism）同时对图像和问句进行注意力加权。Yu等人~\cite{yu2018beyond}通过在问句中使用自注意力机制对协同注意力机制进行简化。早期的协同注意力机制只是在单个模态下分别进行注意力加权，之后，Nguyen等人~\cite{nguyen2018improved}、Kim等人~\cite{kim2018bilinear}、Gao等人~\cite{gao2019dynamic}、Yu等人~\cite{yu2019deep}分别提出“密集型”协同注意力机制，充分计算每一个单词特征和每一个图像视觉特征之间的交互，进一步提升视觉问答性能。


\subsection{视觉问答模型的文本偏置}

尽管所有的视觉问答模型都在融合了多模态的特征之后进行答案预测，但是大量的研究工作表明~\cite{jabri2016revisiting,agrawal2016analyzing,zhang2016yin,goyal2017making}，目前的视觉问答模型在预测答案时往往过于依赖文本偏置。为了减少文本偏置对视觉问答的影响，主要有两种解决方案：

（1）收集更平衡的数据集：减少文本偏置最简单的方法就是收集更加平衡的数据集。例如：Zhang等人~\cite{zhang2016yin}通过对抽象数据集中所有的判断题收集互补的场景图像，使得判断题的答案刚好完全相反。Goyal等人~\cite{goyal2017making}对真实图像数据集的所有类型的问题都收集互补的图像，使得标准答案与原始图像不同。虽然这些数据集在一定程度上可以缓解文本偏置的问题，但是由于训练集和测试集的问题答案分布基本一致，目前的模型仍然可以通过统计的偏置得到较高的准确率~\cite{agrawal2018don}。例如，在数据集VQA-CP中，当训练集和测试集的问题答案分布不同时，模型都会出现明显的性能下降。本文，我们同样是参考这个思路，来生成更多的互补样本。相反，本文提出的反事实样本生成机制不需要任何额外的人工标注。

（2）设计更合理的去偏置模型：另一种减少文本偏置的方法就是设计更合理的去偏置模型。到目前为止，效果最后的去偏置模型就是复合模型~\cite{ramakrishnan2018overcoming,grand2019adversarial,belinkov2019don,cadene2019rubi,clark2019don,mahabadi2019simple}：通过引入一个辅助网络来约束视觉问答网络的训练，其中的辅助网络只用文本（即问题语句）作为输入。在本文，我们提出的反事实样本生成机制可以无缝地应用于复合模型中，进一步减少文本偏置的影响。


\subsection{视觉问答模型的特性}

视觉可解释性和问题敏感性是一个理想的视觉问答模型必不可少的两个特性：

\textbf{\kaishu{视觉可解释性}}：为了提升视觉可解释性，早期的视觉问答模型~\cite{qiao2018exploring,liu2017attention,zhang2019interpretable}都直接使用人类关注的区域作为额外的监督信息。然而，由于强大的文本偏置，即使模块关注到正确的图像区域，后续的网络结构仍然会忽视这部分视觉信号~\cite{selvaraju2019taking}。因此，最近的一些视觉问答工作~\cite{selvaraju2019taking,wu2019self}开始使用工具Grad-CAM~\cite{selvaraju2017grad}来计算每个物体对正确答案的贡献，然后让不同物体的贡献度与人工标注的一致。但是，这类方法有两个明显的缺点：1）它们需要额外的人工标注，即每个物体的贡献度大小或者排序；2）目前这类方法都不是端到端训练的。

\textbf{\kaishu{问题敏感性}}：如果一个视觉问答模型能够充分理解问题的含义，那么这个模型应该对问题的变化十分的敏感，即具备问题敏感性。据我们所知，目前只有一个工作讨论了视觉问答模型的问题敏感性~\cite{shah2019cycle}。具体来说，Shah等人~\cite{shah2019cycle}对于视觉问答和视觉问题生成两个对偶任务设计了一个循环一致的损失函数，然后通过引入随机噪声来生成不同的问题。这个方法只考虑了模型对不同语句表达形式的鲁棒性。相反，本文提出的反事实样本生成机制不仅可以提升模型对不同表达形式的鲁棒性，同时可以帮助模型感知重要单词的改变，提升问题敏感性。

